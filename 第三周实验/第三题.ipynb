{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三题：实现决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验内容：  \n",
    "使用LendingClub Safe Loans数据集：\n",
    "1. 实现信息增益、信息增益率、基尼指数三种划分标准\n",
    "2. 使用给定的训练集完成三种决策树的训练过程\n",
    "3. 计算三种决策树在最大深度为10时在训练集和测试集上的精度，查准率，查全率，F1值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分，我们会实现一个很简单的二叉决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入类库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "loans = pd.read_csv('data/lendingclub/lending-club-data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据中有两列是我们想预测的指标，一项是safe_loans，一项是bad_loans，分别表示正例和负例，我们对其进行处理，将正例的safe_loans设为1，负例设为-1，删除bad_loans这列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对数据进行预处理，将safe_loans作为标记\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "del loans['bad_loans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们只使用grade, term, home_ownership, emp_length这四列作为特征，safe_loans作为标记，只保留loans中的这五列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看前五行数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>term</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>3 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grade        term home_ownership emp_length  safe_loans\n",
       "0     B   36 months           RENT  10+ years           1\n",
       "1     C   60 months           RENT   < 1 year          -1\n",
       "2     C   36 months           RENT  10+ years           1\n",
       "3     C   36 months           RENT  10+ years           1\n",
       "4     A   36 months           RENT    3 years           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "loans = shuffle(loans, random_state = 34)\n",
    "\n",
    "split_line = int(len(loans) * 0.6)\n",
    "train_data = loans.iloc[: split_line]\n",
    "test_data = loans.iloc[split_line:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到所有的特征都是离散类型的特征，需要对数据进行预处理，使用one-hot编码对其进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot编码的思想就是将离散特征变成向量，假设特征$A$有三种取值$\\{a, b, c\\}$，这三种取值等价，如果我们使用1,2,3三个数字表示这三种取值，那么在计算时就会产生偏差，有一些涉及距离度量的算法会认为，2和1离得近，3和1离得远，但这三个值应该是等价的，这种表示方法会造成模型在判断上出现偏差。解决方案就是使用一个三维向量表示他们，用$[1, 0, 0]$表示a，$[0, 1, 0]$表示b，$[0, 0, 1]$表示c，这样三个向量之间的距离就都是相等的了，任意两个向量在欧式空间的距离都是$\\sqrt{2}$。这就是one-hot编码是思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas中使用get_dummies生成one-hot向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(data, features_categorical):\n",
    "    '''\n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "    \n",
    "    features_categorical: list(str)\n",
    "    '''\n",
    "    \n",
    "    # 对所有的离散特征遍历\n",
    "    for cat in features_categorical:\n",
    "        \n",
    "        # 对这列进行one-hot编码，前缀为这个变量名\n",
    "        one_encoding = pd.get_dummies(data[cat], prefix = cat)\n",
    "        \n",
    "        # 将生成的one-hot编码与之前的dataframe拼接起来\n",
    "        data = pd.concat([data, one_encoding],axis=1)\n",
    "        \n",
    "        # 删除掉原始的这列离散特征\n",
    "        del data[cat]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对训练集生成one-hot向量，然后对测试集生成one-hot向量，这里需要注意的是，如果训练集中，特征$A$的取值为$\\{a, b, c\\}$，这样我们生成的特征就有三列，分别为$A\\_a$, $A\\_b$, $A\\_c$，然后我们使用这个训练集训练模型，模型就就会考虑这三个特征，在测试集中如果有一个样本的特征$A$的值为$d$，那它的$A\\_a$，$A\\_b$，$A\\_c$就都为0，我们不去考虑$A\\_d$，因为这个特征在训练模型的时候是不存在的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = one_hot_encoding(train_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "      <th>emp_length_n/a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84320</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121308</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58376</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66430</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65344</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  \\\n",
       "84320            1        0        1        0        0        0        0   \n",
       "121308          -1        1        0        0        0        0        0   \n",
       "58376           -1        0        0        0        1        0        0   \n",
       "66430            1        1        0        0        0        0        0   \n",
       "65344            1        0        0        1        0        0        0   \n",
       "\n",
       "        grade_G  term_ 36 months  term_ 60 months       ...        \\\n",
       "84320         0                1                0       ...         \n",
       "121308        0                1                0       ...         \n",
       "58376         0                1                0       ...         \n",
       "66430         0                0                1       ...         \n",
       "65344         0                1                0       ...         \n",
       "\n",
       "        emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
       "84320                    0                   0                   0   \n",
       "121308                   0                   0                   1   \n",
       "58376                    0                   0                   0   \n",
       "66430                    1                   0                   0   \n",
       "65344                    0                   0                   0   \n",
       "\n",
       "        emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
       "84320                    0                   0                   1   \n",
       "121308                   0                   0                   0   \n",
       "58376                    0                   0                   0   \n",
       "66430                    0                   0                   0   \n",
       "65344                    0                   0                   0   \n",
       "\n",
       "        emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \\\n",
       "84320                    0                   0                    0   \n",
       "121308                   0                   0                    0   \n",
       "58376                    0                   0                    1   \n",
       "66430                    0                   0                    0   \n",
       "65344                    0                   0                    0   \n",
       "\n",
       "        emp_length_n/a  \n",
       "84320                0  \n",
       "121308               0  \n",
       "58376                0  \n",
       "66430                0  \n",
       "65344                0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取所有特征的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year',\n",
       " 'emp_length_n/a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_features = train_data.columns.tolist()\n",
    "one_hot_features.remove(target)\n",
    "one_hot_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是对测试集进行one_hot编码，但只要保留出现在one_hot_features中的特征即可·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_tmp = one_hot_encoding(test_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个空的DataFrame\n",
    "test_data = pd.DataFrame(columns = train_data.columns)\n",
    "for feature in train_data.columns:\n",
    "    # 如果训练集中当前特征在test_data_tmp中出现了，将其复制到test_data中\n",
    "    if feature in test_data_tmp.columns:\n",
    "        test_data[feature] = test_data_tmp[feature].copy()\n",
    "    else:\n",
    "        # 否则就用全为0的列去替代\n",
    "        test_data[feature] = np.zeros(test_data_tmp.shape[0], dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "      <th>emp_length_n/a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37225</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101585</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97692</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88181</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  \\\n",
       "37225            1        0        0        0        0        1        0   \n",
       "101585          -1        0        1        0        0        0        0   \n",
       "31865            1        1        0        0        0        0        0   \n",
       "97692            1        0        0        1        0        0        0   \n",
       "88181            1        1        0        0        0        0        0   \n",
       "\n",
       "        grade_G  term_ 36 months  term_ 60 months       ...        \\\n",
       "37225         0                1                0       ...         \n",
       "101585        0                1                0       ...         \n",
       "31865         0                1                0       ...         \n",
       "97692         0                1                0       ...         \n",
       "88181         0                1                0       ...         \n",
       "\n",
       "        emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
       "37225                    1                   0                   0   \n",
       "101585                   0                   0                   1   \n",
       "31865                    0                   0                   0   \n",
       "97692                    0                   0                   0   \n",
       "88181                    0                   0                   0   \n",
       "\n",
       "        emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
       "37225                    0                   0                   0   \n",
       "101585                   0                   0                   0   \n",
       "31865                    0                   0                   0   \n",
       "97692                    0                   0                   0   \n",
       "88181                    0                   1                   0   \n",
       "\n",
       "        emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \\\n",
       "37225                    0                   0                    0   \n",
       "101585                   0                   0                    0   \n",
       "31865                    0                   0                    0   \n",
       "97692                    0                   0                    0   \n",
       "88181                    0                   0                    0   \n",
       "\n",
       "        emp_length_n/a  \n",
       "37225                0  \n",
       "101585               0  \n",
       "31865                0  \n",
       "97692                0  \n",
       "88181                0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73564, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49043, 26)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集有37224个样本，测试集有9284个样本，**处理完后，所有的特征都是0和1，标记是1和-1**，以上就是数据预处理流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 实现3种特征划分准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树中有很多常用的特征划分方法，比如信息增益、信息增益率、基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要实现一个函数，它的作用是，给定决策树的某个结点内的所有样本的标记，让它计算出对应划分指标的值是多少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们会实现上述三种划分指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里我们约定，将所有特征取值为0的样本，划分到左子树，特征取值为1的样本，划分到右子树**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 信息增益\n",
    "信息熵：\n",
    "$$\n",
    "\\mathrm{Ent}(D) = - \\sum^{\\vert \\mathcal{Y} \\vert}_{k = 1} p_k \\mathrm{log}_2 p_k\n",
    "$$\n",
    "\n",
    "信息增益：\n",
    "$$\n",
    "\\mathrm{Gain}(D, a) = \\mathrm{Ent}(D) - \\sum^{V}_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Ent}(D^v)\n",
    "$$\n",
    "\n",
    "计算信息熵时约定：若$p = 0$，则$p \\log_2p = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面的函数需要填写两个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def information_entropy(labels_in_node):\n",
    "    '''\n",
    "    求当前结点的信息熵\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    labels_in_node: np.ndarray, 如[-1, 1, -1, 1, 1]\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    float: information entropy\n",
    "    '''\n",
    "    # 统计样本总个数\n",
    "    num_of_samples = labels_in_node.shape[0]\n",
    "    \n",
    "    if num_of_samples == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 统计出标记为1的个数\n",
    "    num_of_positive = len(labels_in_node[labels_in_node == 1])\n",
    "    \n",
    "    # 统计出标记为-1的个数\n",
    "    num_of_negative = len(labels_in_node[labels_in_node == -1])                                                                  \n",
    "    \n",
    "    # 统计正例的概率\n",
    "    prob_positive = num_of_positive / num_of_samples\n",
    "    \n",
    "    # 统计负例的概率\n",
    "    prob_negative = num_of_negative / num_of_samples                                                          \n",
    "    \n",
    "    if prob_positive == 0:\n",
    "        positive_part = 0\n",
    "    else:\n",
    "        positive_part = prob_positive * np.log2(prob_positive)\n",
    "    \n",
    "    if prob_negative == 0:\n",
    "        negative_part = 0\n",
    "    else:\n",
    "        negative_part = prob_negative * np.log2(prob_negative)\n",
    "    \n",
    "    return - ( positive_part + negative_part )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是6个测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970950594455\n",
      "0.863120568567\n",
      "0.863120568567\n",
      "0.997502546369\n",
      "-0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 信息熵测试样例1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.97095\n",
    "\n",
    "# 信息熵测试样例2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.86312\n",
    "    \n",
    "# 信息熵测试样例3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.86312\n",
    "\n",
    "# 信息熵测试样例4\n",
    "example_labels = np.array([-1] * 9 + [1] * 8)\n",
    "print(information_entropy(example_labels)) # 0.99750\n",
    "\n",
    "# 信息熵测试样例5\n",
    "example_labels = np.array([1] * 8)\n",
    "print(information_entropy(example_labels)) # 0\n",
    "\n",
    "# 信息熵测试样例6\n",
    "example_labels = np.array([])\n",
    "print(information_entropy(example_labels)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来完成计算所有特征的信息增益的函数  \n",
    "**需要填写三个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_information_gains(data, features, target, annotate = False):\n",
    "    '''\n",
    "    计算所有特征的信息增益\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "        data: pd.DataFrame，传入的样本，带有特征和标记的dataframe\n",
    "        \n",
    "        features: list(str)，特征名组成的list\n",
    "        \n",
    "        target: str, 标记(label)的名字\n",
    "        \n",
    "        annotate, boolean，是否打印所有特征的信息增益值，默认为False\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "        information_gains: dict, key: str, 特征名\n",
    "                                 value: float，信息增益\n",
    "    '''\n",
    "    \n",
    "    # 我们将每个特征划分的信息增益值存储在一个dict中\n",
    "    # 键是特征名，值是信息增益值\n",
    "    information_gains = dict()\n",
    "    \n",
    "    # 对所有的特征进行遍历，使用信息增益对每个特征进行计算\n",
    "    for feature in features:\n",
    "        \n",
    "        # 左子树保证所有的样本的这个特征取值为0\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        # 右子树保证所有的样本的这个特征取值为1\n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # 计算左子树的信息熵\n",
    "        left_entropy = information_entropy(left_split_target)\n",
    "        \n",
    "        # 计算左子树的权重\n",
    "        left_weight = len(left_split_target) / ((len(left_split_target) + len(right_split_target)))\n",
    "\n",
    "        # 计算右子树的信息熵\n",
    "        right_entropy = information_entropy(right_split_target)\n",
    "        \n",
    "        # 计算右子树的权重\n",
    "        right_weight = len(right_split_target) / ((len(right_split_target) + len(left_split_target)))\n",
    "        \n",
    "        # 计算当前结点的信息熵\n",
    "        current_entropy = information_entropy(data[target])\n",
    "            \n",
    "        # 计算使用当前特征划分的信息增益\n",
    "        gain = current_entropy - left_weight * left_entropy - right_weight * right_entropy\n",
    "        \n",
    "        \n",
    "        # 将特征名与增益值以键值对的形式存储在information_gains中\n",
    "        information_gains[feature] = gain\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gain)\n",
    "            \n",
    "    return information_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0175919801789\n",
      "0.0142918503294\n",
      "0.00370492003453\n"
     ]
    }
   ],
   "source": [
    "# 信息增益测试样例1\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['grade_A']) # 0.01759\n",
    "\n",
    "# 信息增益测试样例2\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['term_ 60 months']) # 0.01429\n",
    "\n",
    "# 信息增益测试样例3\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['grade_B']) # 0.00370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 信息增益率\n",
    "信息增益率：\n",
    "\n",
    "$$\n",
    "\\mathrm{Gain\\_ratio}(D, a) = \\frac{\\mathrm{Gain}(D, a)}{\\mathrm{IV}(a)}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\mathrm{IV}(a) = - \\sum^V_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\log_2 \\frac{\\vert D^v \\vert}{\\vert D \\vert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成计算所有特征信息增益率的函数  \n",
    "**这里要完成五个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_information_gain_ratios(data, features, target, annotate = False):\n",
    "    '''\n",
    "    计算所有特征的信息增益率并保存起来\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame, 带有特征和标记的数据\n",
    "    \n",
    "    features: list(str)，特征名组成的list\n",
    "    \n",
    "    target: str， 特征的名字\n",
    "    \n",
    "    annotate: boolean, default False，是否打印注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    gain_ratios: dict, key: str, 特征名\n",
    "                       value: float，信息增益率\n",
    "    '''\n",
    "    \n",
    "    gain_ratios = dict()\n",
    "    \n",
    "    # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算\n",
    "    for feature in features:\n",
    "        \n",
    "        # 左子树保证所有的样本的这个特征取值为0\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        # 右子树保证所有的样本的这个特征取值为1\n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # 计算左子树的信息熵\n",
    "        left_entropy = information_entropy(left_split_target)\n",
    "        \n",
    "        # 计算左子树的权重\n",
    "        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "\n",
    "        # 计算右子树的信息熵\n",
    "        right_entropy = information_entropy(right_split_target)\n",
    "        \n",
    "        # 计算右子树的权重\n",
    "        right_weight = len(right_split_target) / (len(right_split_target) + len(left_split_target))\n",
    "        \n",
    "        # 计算当前结点的信息熵\n",
    "        current_entropy = information_entropy(data[target])\n",
    "        \n",
    "        # 计算当前结点的信息增益\n",
    "        \n",
    "        gain = current_entropy - left_weight * left_entropy - right_weight * right_entropy\n",
    "        \n",
    "        # 计算IV公式中，当前特征为0的值\n",
    "        if left_weight == 0:\n",
    "            left_IV = 0\n",
    "        else:\n",
    "            left_IV = left_weight * np.log2(left_weight)\n",
    "        \n",
    "        \n",
    "        # 计算IV公式中，当前特征为1的值\n",
    "        if right_weight == 0:\n",
    "            right_IV = 0\n",
    "        else:\n",
    "            right_IV = right_weight * np.log2(right_weight)\n",
    "        \n",
    "        # IV 等于所有子树IV之和的相反数\n",
    "        IV = - (left_IV + right_IV)\n",
    "            \n",
    "        # 计算使用当前特征划分的信息增益率\n",
    "        # 这里为了防止IV是0，导致除法得到np.inf（无穷），在分母加了一个很小的小数\n",
    "        gain_ratio = gain / (IV + np.finfo(np.longdouble).eps)\n",
    "        \n",
    "        # 信息增益率的存储\n",
    "        gain_ratios[feature] = gain_ratio\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gain_ratio)\n",
    "            \n",
    "    return gain_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025734780668\n",
      "0.00417549506943\n",
      "0.0197093627186\n"
     ]
    }
   ],
   "source": [
    "# 信息增益率测试样例1\n",
    "print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_A']) # 0.02573\n",
    "\n",
    "# 信息增益率测试样例2\n",
    "print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_B']) # 0.00417\n",
    "\n",
    "# 信息增益率测试样例3\n",
    "print(compute_information_gain_ratios(train_data, one_hot_features, target)['term_ 60 months']) # 0.01970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 基尼指数\n",
    "数据集$D$的基尼值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Gini}(D) & = \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} \\sum_{k' \\neq k} p_k p_{k'}\\\\\n",
    "& = 1 - \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} p^2_k.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "属性$a$的基尼指数：\n",
    "\n",
    "$$\n",
    "\\mathrm{Gini\\_index}(D, a) = \\sum^V_{v = 1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Gini}(D^v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成数据集基尼值的计算  \n",
    "**这里需要填写三部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(labels_in_node):\n",
    "    '''\n",
    "    计算一个结点内样本的基尼指数\n",
    "    \n",
    "    Paramters\n",
    "    ----------\n",
    "    label_in_data: np.ndarray, 样本的标记，如[-1, -1, 1, 1, 1]\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    gini: float，基尼指数\n",
    "    '''\n",
    "    \n",
    "    # 统计样本总个数\n",
    "    num_of_samples = labels_in_node.shape[0]\n",
    "    \n",
    "    if num_of_samples == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 统计出1的个数\n",
    "    num_of_positive = len(labels_in_node[labels_in_node == 1])\n",
    "    \n",
    "    # 统计出-1的个数\n",
    "    num_of_negative = len(labels_in_node[labels_in_node == -1])\n",
    "    \n",
    "    # 统计正例的概率\n",
    "    prob_positive = num_of_positive / num_of_samples\n",
    "    \n",
    "    # 统计负例的概率\n",
    "    prob_negative = num_of_negative / num_of_samples\n",
    "    \n",
    "    # 计算基尼值\n",
    "    # YOUR CODE HERE\n",
    "    gini = 1 - prob_positive ** 2 - prob_negative ** 2\n",
    "    \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.40816326530612246\n",
      "0.40816326530612246\n",
      "0.49826989619377154\n",
      "0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 基尼值测试样例1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "print(gini(example_labels)) # 0.48\n",
    "\n",
    "# 基尼值测试样例2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "print(gini(example_labels)) # 0.40816\n",
    "    \n",
    "# 基尼值测试样例3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "print(gini(example_labels)) # 0.40816\n",
    "\n",
    "# 基尼值测试样例4\n",
    "example_labels = np.array([-1] * 9 + [1] * 8)\n",
    "print(gini(example_labels)) # 0.49827\n",
    "\n",
    "# 基尼值测试样例5\n",
    "example_labels = np.array([1] * 8)\n",
    "print(gini(example_labels)) # 0\n",
    "\n",
    "# 基尼值测试样例6\n",
    "example_labels = np.array([])\n",
    "print(gini(example_labels)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后计算所有特征的基尼指数  \n",
    "**这里需要填写三部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gini_indices(data, features, target, annotate = False):\n",
    "    '''\n",
    "    计算使用各个特征进行划分时，各特征的基尼指数\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame, 带有特征和标记的数据\n",
    "    \n",
    "    features: list(str)，特征名组成的list\n",
    "    \n",
    "    target: str， 特征的名字\n",
    "    \n",
    "    annotate: boolean, default False，是否打印注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    gini_indices: dict, key: str, 特征名\n",
    "                       value: float，基尼指数\n",
    "    '''\n",
    "    \n",
    "    gini_indices = dict()\n",
    "    # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算\n",
    "    for feature in features:\n",
    "        # 左子树保证所有的样本的这个特征取值为0\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        # 右子树保证所有的样本的这个特征取值为1\n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # 计算左子树的基尼值\n",
    "        left_gini = gini(left_split_target)\n",
    "        \n",
    "        # 计算左子树的权重\n",
    "        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "\n",
    "        # 计算右子树的基尼值\n",
    "        right_gini = gini(right_split_target)\n",
    "        \n",
    "        # 计算右子树的权重\n",
    "        right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "        \n",
    "        # 计算当前结点的基尼指数\n",
    "        gini_index = left_gini * left_weight + right_gini * right_weight\n",
    "        \n",
    "        # 存储\n",
    "        gini_indices[feature] = gini_index\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gini_index)\n",
    "            \n",
    "    return gini_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30095209649643617\n",
      "0.3056855375882364\n",
      "0.30055418611740065\n"
     ]
    }
   ],
   "source": [
    "# 基尼指数测试样例1\n",
    "print(compute_gini_indices(train_data, one_hot_features, target)['grade_A']) # 0.30095\n",
    "\n",
    "# 基尼指数测试样例2\n",
    "print(compute_gini_indices(train_data, one_hot_features, target)['grade_B']) # 0.30568\n",
    "\n",
    "# 基尼指数测试样例3\n",
    "print(compute_gini_indices(train_data, one_hot_features, target)['term_ 36 months']) # 0.30055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 完成最优特征的选择 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此，我们完成了三种划分策略的实现，接下来就是完成获取最优特征的函数  \n",
    "**这里需要填写三个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, criterion = 'gini', annotate = False):\n",
    "    '''\n",
    "    给定划分方法和数据，找到最优的划分特征\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame, 带有特征和标记的数据\n",
    "    \n",
    "    features: list(str)，特征名组·成的list\n",
    "    \n",
    "    target: str， 特征的名字\n",
    "    \n",
    "    criterion: str, 使用哪种指标，三种选项: 'information_gain', 'gain_ratio', 'gini'\n",
    "    \n",
    "    annotate: boolean, default False，是否打印注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    best_feature: str, 最佳的划分特征的名字\n",
    "    \n",
    "    '''\n",
    "    if criterion == 'information_gain':\n",
    "        if annotate:\n",
    "            print('using information gain')\n",
    "        \n",
    "        # 得到当前所有特征的信息增益\n",
    "        information_gains = compute_information_gains(data, features, target, annotate)\n",
    "    \n",
    "        # information_gains是一个dict类型的对象，我们要找值最大的那个元素的键是谁\n",
    "        \n",
    "        \n",
    "        # 根据这些特征和他们的信息增益，找到最佳的划分特征\n",
    "        best_feature = max(information_gains, key=information_gains.get)\n",
    "        \n",
    "        return best_feature\n",
    "\n",
    "    elif criterion == 'gain_ratio':\n",
    "        if annotate:\n",
    "            print('using information gain ratio')\n",
    "        \n",
    "        # 得到当前所有特征的信息增益率\n",
    "        gain_ratios = compute_information_gain_ratios(data, features, target, annotate)\n",
    "    \n",
    "        # 根据这些特征和他们的信息增益率，找到最佳的划分特征\n",
    "        best_feature = max(gain_ratios, key=gain_ratios.get)\n",
    "\n",
    "        return best_feature\n",
    "    \n",
    "    elif criterion == 'gini':\n",
    "        if annotate:\n",
    "            print('using gini')\n",
    "        \n",
    "        # 得到当前所有特征的基尼指数\n",
    "        gini_indices = compute_gini_indices(data, features, target, annotate)\n",
    "        \n",
    "        # 根据这些特征和他们的基尼指数，找到最佳的划分特征\n",
    "        best_feature = max(gini_indices, key=gini_indices.get)\n",
    "\n",
    "        return best_feature\n",
    "    else:\n",
    "        raise Exception(\"传入的criterion不合规!\", criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 判断结点内样本的类别是否为同一类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里需要填写两个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    '''\n",
    "    求树的结点中，样本数少的那个类的样本有多少，比如输入是[1, 1, -1, -1, 1]，返回2\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    labels_in_node: np.ndarray, pd.Series\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    int：个数\n",
    "    \n",
    "    '''\n",
    "    # 如果传入的array为空，返回0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 统计1的个数\n",
    "    num_of_one = len(labels_in_node[labels_in_node == 1])\n",
    "    \n",
    "    # 统计-1的个数\n",
    "    num_of_minus_one = len(labels_in_node[labels_in_node == -1])\n",
    "    \n",
    "    return num_of_one if num_of_minus_one > num_of_one else num_of_minus_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 测试样例1\n",
    "print(intermediate_node_num_mistakes(np.array([1, 1, -1, -1, -1]))) # 2\n",
    "\n",
    "# 测试样例2\n",
    "print(intermediate_node_num_mistakes(np.array([]))) # 0\n",
    "\n",
    "# 测试样例3\n",
    "print(intermediate_node_num_mistakes(np.array([1]))) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 创建叶子结点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    '''\n",
    "    计算出当前叶子结点的标记是什么，并且将叶子结点信息保存在一个dict中\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    target_values: pd.Series, 当前叶子结点内样本的标记\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    leaf: dict，表示一个叶结点，\n",
    "            leaf['splitting_features'], None，叶结点不需要划分特征\n",
    "            leaf['left'], None，叶结点没有左子树\n",
    "            leaf['right'], None，叶结点没有右子树\n",
    "            leaf['is_leaf'], True, 是否是叶子结点\n",
    "            leaf['prediction'], int, 表示该叶子结点的预测值\n",
    "    '''\n",
    "    # 创建叶子结点\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True}\n",
    "   \n",
    "    # 数结点内-1和+1的个数\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])    \n",
    "\n",
    "    # 叶子结点的标记使用少数服从多数的原则，为样本数多的那类的标记，保存在 leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = 1\n",
    "    else:\n",
    "        leaf['prediction'] = -1\n",
    "\n",
    "    # 返回叶子结点\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 递归地创建决策树\n",
    "递归的创建决策树  \n",
    "递归算法终止的三个条件：\n",
    "1. 如果结点内所有的样本的标记都相同，该结点就不需要再继续划分，直接做叶子结点即可\n",
    "2. 如果结点所有的特征都已经在之前使用过了，在当前结点无剩余特征可供划分样本，该结点直接做叶子结点\n",
    "3. 如果当前结点的深度已经达到了我们限制的树的最大深度，直接做叶子结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里需要填写七个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, criterion = 'gini', current_depth = 0, max_depth = 10, annotate = False):\n",
    "    '''\n",
    "    Parameter:\n",
    "    ----------\n",
    "    data: pd.DataFrame, 数据\n",
    "\n",
    "    features: iterable, 特征组成的可迭代对象，比如一个list\n",
    "\n",
    "    target: str, 标记的名字\n",
    "\n",
    "    criterion: 'str', 特征划分方法，只支持三种：'information_gain', 'gain_ratio', 'gini'\n",
    "\n",
    "    current_depth: int, 当前深度，递归的时候需要记录\n",
    "\n",
    "    max_depth: int, 树的最大深度，我们设定的树的最大深度，达到最大深度需要终止递归\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict, dict['is_leaf']          : False, 当前顶点不是叶子结点\n",
    "          dict['prediction']       : None, 不是叶子结点就没有预测值\n",
    "          dict['splitting_feature']: splitting_feature, 当前结点是使用哪个特征进行划分的\n",
    "          dict['left']             : dict\n",
    "          dict['right']            : dict\n",
    "    '''\n",
    "    \n",
    "    if criterion not in ['information_gain', 'gain_ratio', 'gini']:\n",
    "        raise Exception(\"传入的criterion不合规!\", criterion)\n",
    "    \n",
    "    # 复制一份特征，存储起来，每使用一个特征进行划分，我们就删除一个\n",
    "    remaining_features = features[:]\n",
    "    \n",
    "    # 取出标记值\n",
    "    target_values = data[target]\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "\n",
    "    # 终止条件1\n",
    "    # 如果当前结点内所有样本同属一类，即这个结点中，各类别样本数最小的那个等于0\n",
    "    # 使用前面写的intermediate_node_num_mistakes来完成这个判断\n",
    "    \n",
    "    if intermediate_node_num_mistakes(target_values) == 0:\n",
    "        # intermediate_node_num_mistakes(data)\n",
    "        print(\"Stopping condition 1 reached.\")\n",
    "        return create_leaf(target_values)   # 创建叶子结点\n",
    "    \n",
    "    # 终止条件2\n",
    "    # 如果已经没有剩余的特征可供分割，即remaining_features为空\n",
    "    if remaining_features == None or len(remaining_features) == 0:\n",
    "        print(\"Stopping condition 2 reached.\")\n",
    "        return create_leaf(target_values)   # 创建叶子结点\n",
    "    \n",
    "    # 终止条件3\n",
    "    # 如果已经到达了我们要求的最大深度，即当前深度达到了最大深度\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values)   # 创建叶子结点\n",
    "\n",
    "    # 找到最优划分特征\n",
    "    # 使用best_splitting_feature这个函数\n",
    "    \n",
    "    splitting_feature = best_splitting_feature(data, features, target, criterion, annotate)\n",
    "    \n",
    "    # 使用我们找到的最优特征将数据划分成两份\n",
    "    # 左子树的数据\n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    \n",
    "    # 右子树的数据\n",
    "    right_split = data[data[splitting_feature] != 0]\n",
    "    \n",
    "    # 现在已经完成划分，我们要从剩余特征中删除掉当前这个特征\n",
    "    remaining_features.remove(splitting_feature)\n",
    "    \n",
    "    # 打印当前划分使用的特征，打印左子树样本个数，右子树样本个数\n",
    "    print(\"Split on feature %s. (%s, %s)\" % (splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # 如果使用当前的特征，将所有的样本都划分到一棵子树中，那么就直接将这棵子树变成叶子结点\n",
    "    # 判断左子树是不是“完美”的\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target])\n",
    "    \n",
    "    # 判断右子树是不是“完美”的\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating right node.\")\n",
    "        return create_leaf(right_split[target])\n",
    "\n",
    "    # 递归地创建左子树\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate)\n",
    "    \n",
    "    # 递归地创建右子树\n",
    "    \n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate)\n",
    "\n",
    "    # 返回树的非叶子结点\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature emp_length_5 years. (67854, 5710)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (67854 data points).\n",
      "Split on feature emp_length_< 1 year. (61228, 6626)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (61228 data points).\n",
      "Split on feature home_ownership_OWN. (56181, 5047)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (56181 data points).\n",
      "Split on feature emp_length_4 years. (51399, 4782)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (51399 data points).\n",
      "Split on feature emp_length_9 years. (48916, 2483)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (48916 data points).\n",
      "Split on feature emp_length_8 years. (45939, 2977)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (45939 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2977 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2483 data points).\n",
      "Split on feature emp_length_1 year. (2483, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (4782 data points).\n",
      "Split on feature emp_length_1 year. (4782, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (5047 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (5047, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (6626 data points).\n",
      "Split on feature emp_length_1 year. (6626, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (5710 data points).\n",
      "Split on feature emp_length_1 year. (5710, 0)\n",
      "Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "my_decision_tree = decision_tree_create(train_data, one_hot_features, target, 'gini', max_depth = 6, annotate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，模型就训练好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要完成预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):\n",
    "    '''\n",
    "    递归的进行预测，一次只能预测一个样本\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dict\n",
    "    \n",
    "    x: pd.Series，待预测的样本\n",
    "    \n",
    "    annotate： boolean, 是否显示注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    返回预测的标记\n",
    "    '''\n",
    "    \n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "            print (\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate:\n",
    "             print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们取测试集第一个样本来测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample = test_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True class: 1 \n",
      "Predicted class: 1 \n"
     ]
    }
   ],
   "source": [
    "print('True class: %s ' % (test_sample['safe_loans']))\n",
    "print('Predicted class: %s ' % classify(my_decision_tree, test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印出使用决策树判断的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on emp_length_5 years = 0\n",
      "Split on emp_length_< 1 year = 0\n",
      "Split on home_ownership_OWN = 0\n",
      "Split on emp_length_4 years = 0\n",
      "Split on emp_length_9 years = 0\n",
      "Split on emp_length_8 years = 0\n",
      "At leaf, predicting 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(my_decision_tree, test_sample, annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 在测试集上对我们的模型进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先来编写一个批量预测的函数，传入的是整个测试集那样的pd.DataFrame，这个函数返回一个np.ndarray，存储模型的预测结果  \n",
    "**这里需要填写一个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(tree, data):\n",
    "    '''\n",
    "    按行遍历data，对每个样本进行预测，将值存在prediction中，最后返回np.ndarray\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    tree: dict, 模型\n",
    "    \n",
    "    data: pd.DataFrame, 数据\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    predictions：np.ndarray, 模型对这些样本的预测结果\n",
    "    '''\n",
    "    predictions = np.zeros(len(data)) # 长度和data一样\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        predictions[i] = classify(tree, data.iloc[i])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内\n",
    "**树的最大深度为6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini\n",
      "0.81224639602 0.81224639602 1.0 0.89639730867\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature grade_A. (60204, 13360)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (60204 data points).\n",
      "Split on feature grade_B. (37768, 22436)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (37768 data points).\n",
      "Split on feature grade_C. (19878, 17890)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (19878 data points).\n",
      "Split on feature term_ 36 months. (8812, 11066)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (8812 data points).\n",
      "Split on feature home_ownership_RENT. (5438, 3374)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5438 data points).\n",
      "Split on feature grade_D. (3299, 2139)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3299 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2139 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (3374 data points).\n",
      "Split on feature grade_D. (2198, 1176)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2198 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1176 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (11066 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (6987, 4079)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6987 data points).\n",
      "Split on feature grade_F. (6650, 337)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6650 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (337 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4079 data points).\n",
      "Split on feature grade_G. (4003, 76)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4003 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (76 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (17890 data points).\n",
      "Split on feature emp_length_n/a. (17321, 569)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (17321 data points).\n",
      "Split on feature term_ 36 months. (3753, 13568)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (3753 data points).\n",
      "Split on feature home_ownership_RENT. (2696, 1057)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2696 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1057 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (13568 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (7920, 5648)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7920 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (5648 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (569 data points).\n",
      "Split on feature home_ownership_RENT. (342, 227)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (342 data points).\n",
      "Split on feature term_ 60 months. (288, 54)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (288 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (54 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (227 data points).\n",
      "Split on feature term_ 36 months. (19, 208)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (19 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (208 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (22436 data points).\n",
      "Split on feature term_ 36 months. (1934, 20502)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (1934 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (689, 1245)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (689 data points).\n",
      "Split on feature emp_length_7 years. (647, 42)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (647 data points).\n",
      "Split on feature emp_length_10+ years. (483, 164)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (483 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (164 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (42 data points).\n",
      "Split on feature home_ownership_RENT. (8, 34)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (8 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (34 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1245 data points).\n",
      "Split on feature emp_length_3 years. (1152, 93)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1152 data points).\n",
      "Split on feature emp_length_7 years. (1081, 71)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (71 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (93 data points).\n",
      "Split on feature grade_C. (93, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (20502 data points).\n",
      "Split on feature emp_length_n/a. (19809, 693)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (19809 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (10391, 9418)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (10391 data points).\n",
      "Split on feature home_ownership_OTHER. (10357, 34)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (10357 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (34 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (9418 data points).\n",
      "Split on feature emp_length_< 1 year. (8877, 541)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (8877 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (541 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (693 data points).\n",
      "Split on feature home_ownership_RENT. (434, 259)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (434 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (125, 309)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (125 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (309 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (259 data points).\n",
      "Split on feature grade_C. (259, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (13360 data points).\n",
      "Split on feature emp_length_n/a. (12848, 512)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (12848 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (5651, 7197)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (5651 data points).\n",
      "Split on feature term_ 60 months. (5584, 67)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (5584 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature emp_length_3 years. (5037, 547)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5037 data points).\n",
      "Split on feature emp_length_7 years. (4799, 238)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4799 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (238 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (547 data points).\n",
      "Split on feature home_ownership_OTHER. (545, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (545 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (67 data points).\n",
      "Split on feature home_ownership_RENT. (18, 49)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (18 data points).\n",
      "Split on feature emp_length_10+ years. (12, 6)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (12 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (49 data points).\n",
      "Split on feature emp_length_4 years. (47, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (47 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (7197 data points).\n",
      "Split on feature emp_length_2 years. (6616, 581)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (6616 data points).\n",
      "Split on feature emp_length_4 years. (6134, 482)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6134 data points).\n",
      "Split on feature emp_length_1 year. (5773, 361)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (5773 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (361 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (482 data points).\n",
      "Split on feature term_ 36 months. (14, 468)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (14 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (468 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (581 data points).\n",
      "Split on feature term_ 60 months. (569, 12)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (569 data points).\n",
      "Split on feature grade_B. (569, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Split on feature grade_B. (12, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (512 data points).\n",
      "Split on feature home_ownership_RENT. (371, 141)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (371 data points).\n",
      "Split on feature home_ownership_OWN. (263, 108)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (263 data points).\n",
      "Split on feature term_ 60 months. (256, 7)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (256 data points).\n",
      "Split on feature grade_B. (256, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (7 data points).\n",
      "Split on feature grade_B. (7, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (108 data points).\n",
      "Split on feature term_ 36 months. (2, 106)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_B. (2, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (106 data points).\n",
      "Split on feature grade_B. (106, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (141 data points).\n",
      "Split on feature term_ 36 months. (1, 140)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (140 data points).\n",
      "Split on feature grade_B. (140, 0)\n",
      "Creating leaf node.\n",
      "information_gain\n",
      "0.812238739014 0.81220561548 0.999949792896 0.896372474009\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature grade_F. (71229, 2335)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (71229 data points).\n",
      "Split on feature grade_A. (57869, 13360)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (57869 data points).\n",
      "Split on feature grade_G. (57232, 637)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (57232 data points).\n",
      "Split on feature grade_E. (51828, 5404)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (51828 data points).\n",
      "Split on feature grade_D. (40326, 11502)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (40326 data points).\n",
      "Split on feature emp_length_n/a. (39005, 1321)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (39005 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1321 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (11502 data points).\n",
      "Split on feature emp_length_n/a. (11094, 408)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (11094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (408 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (5404 data points).\n",
      "Split on feature term_ 36 months. (3185, 2219)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (3185 data points).\n",
      "Split on feature home_ownership_OTHER. (3184, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3184 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2219 data points).\n",
      "Split on feature emp_length_1 year. (2011, 208)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2011 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (208 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (637 data points).\n",
      "Split on feature emp_length_3 years. (590, 47)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (590 data points).\n",
      "Split on feature emp_length_n/a. (577, 13)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (577 data points).\n",
      "Split on feature emp_length_2 years. (528, 49)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (528 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (49 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (13 data points).\n",
      "Split on feature home_ownership_RENT. (9, 4)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (9 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (47 data points).\n",
      "Split on feature home_ownership_OTHER. (46, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (46 data points).\n",
      "Split on feature home_ownership_OWN. (44, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (44 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (13360 data points).\n",
      "Split on feature emp_length_n/a. (12848, 512)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (12848 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature term_ 60 months. (12599, 249)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (12599 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (5584, 7015)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5584 data points).\n",
      "Split on feature emp_length_7 years. (5346, 238)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (5346 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (238 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (7015 data points).\n",
      "Split on feature emp_length_2 years. (6446, 569)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6446 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (569 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (249 data points).\n",
      "Split on feature emp_length_9 years. (242, 7)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (242 data points).\n",
      "Split on feature home_ownership_RENT. (193, 49)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (193 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (49 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (7 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (512 data points).\n",
      "Split on feature term_ 60 months. (502, 10)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (502 data points).\n",
      "Split on feature home_ownership_OWN. (396, 106)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (396 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (140, 256)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (140 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (256 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (106 data points).\n",
      "Split on feature grade_B. (106, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (10 data points).\n",
      "Split on feature home_ownership_OWN. (8, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (8 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (1, 7)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_B. (2, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (2335 data points).\n",
      "Split on feature emp_length_7 years. (2197, 138)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (2197 data points).\n",
      "Split on feature term_ 60 months. (478, 1719)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (478 data points).\n",
      "Split on feature emp_length_8 years. (460, 18)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (460 data points).\n",
      "Split on feature emp_length_4 years. (433, 27)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (433 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (287, 146)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (287 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (146 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (27 data points).\n",
      "Split on feature home_ownership_OWN. (25, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (25 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (18 data points).\n",
      "Split on feature home_ownership_OWN. (17, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (17 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (11, 6)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (11 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (1719 data points).\n",
      "Split on feature home_ownership_OTHER. (1717, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1717 data points).\n",
      "Split on feature emp_length_3 years. (1577, 140)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1577 data points).\n",
      "Split on feature emp_length_n/a. (1552, 25)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1552 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (25 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (140 data points).\n",
      "Split on feature home_ownership_RENT. (73, 67)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (73 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (67 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (138 data points).\n",
      "Split on feature term_ 60 months. (29, 109)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (29 data points).\n",
      "Split on feature home_ownership_OWN. (25, 4)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (25 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (13, 12)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (13 data points).\n",
      "Split on feature grade_A. (13, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Split on feature grade_A. (12, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (4 data points).\n",
      "Split on feature grade_A. (4, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (109 data points).\n",
      "Split on feature home_ownership_RENT. (51, 58)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (51 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (8, 43)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (8 data points).\n",
      "Split on feature grade_A. (8, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (43 data points).\n",
      "Split on feature grade_A. (43, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (58 data points).\n",
      "Split on feature grade_A. (58, 0)\n",
      "Creating leaf node.\n",
      "gain_ratio\n",
      "0.812705478753 0.81224639602 0.999096272123 0.896313312164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# gini\n",
    "predictions = predict(my_decision_tree, test_data)\n",
    "accuracy = accuracy_score(test_data['safe_loans'], predictions)\n",
    "precision = precision_score(test_data['safe_loans'], predictions)\n",
    "recall = recall_score(test_data['safe_loans'], predictions)\n",
    "f1 = f1_score(test_data['safe_loans'], predictions)\n",
    "print('gini')\n",
    "print(precision, accuracy, recall, f1)\n",
    "\n",
    "# information_gain\n",
    "my_decision_tree2 = decision_tree_create(train_data, one_hot_features, target, 'information_gain', max_depth = 6, annotate = False)\n",
    "predictions_2 = predict(my_decision_tree2, test_data)\n",
    "accuracy_2 = accuracy_score(test_data['safe_loans'], predictions_2)\n",
    "precision_2 = precision_score(test_data['safe_loans'], predictions_2)\n",
    "recall_2 = recall_score(test_data['safe_loans'], predictions_2)\n",
    "f1_2 = f1_score(test_data['safe_loans'], predictions_2)\n",
    "print('information_gain')\n",
    "print(precision_2, accuracy_2, recall_2, f1_2)\n",
    "\n",
    "# gain_ratio\n",
    "my_decision_tree3 = decision_tree_create(train_data, one_hot_features, target, 'gain_ratio', max_depth = 6, annotate = False)\n",
    "predictions_3 = predict(my_decision_tree3, test_data)\n",
    "accuracy_3 = accuracy_score(test_data['safe_loans'], predictions_3)\n",
    "precision_3 = precision_score(test_data['safe_loans'], predictions_3)\n",
    "recall_3 = recall_score(test_data['safe_loans'], predictions_3)\n",
    "f1_3 = f1_score(test_data['safe_loans'], predictions_3)\n",
    "print('gain_ratio')\n",
    "print(precision_3, accuracy_3, recall_3, f1_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "树的最大深度为6  \n",
    "\n",
    "###### 双击此处编写\n",
    "\n",
    "划分标准|精度|查准率|查全率|F1\n",
    "-|-|-|-|-\n",
    "信息增益|0.812238739014 | 0.81220561548 | 0.999949792896 | 0.896372474009\n",
    "信息增益率|0.812705478753 | 0.81224639602 | 0.999096272123 | 0.896313312164\n",
    "基尼指数|0.81224639602 | 0.81224639602 | 1.0 | 0.89639730867\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 扩展：使用Echarts绘制决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用echarts绘制出我们训练的决策树，这时候可以利用pyecharts这个库\n",
    "[pyecharts](http://pyecharts.org/#/)  \n",
    "pyecharts可以与jupyter notebook无缝衔接，直接在notebook中绘制图表。\n",
    "**提醒：pyecharts还未支持jupyter lab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入树形图\n",
    "from pyecharts import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "echarts中的树形图要求我们提供一组这样的数据  \n",
    "```\n",
    "  [\n",
    "      {\n",
    "          value: 1212,    # 数值\n",
    "          # 子节点\n",
    "          children: [\n",
    "              {\n",
    "                  # 子节点数值\n",
    "                  value: 2323,\n",
    "                  # 子节点名\n",
    "                  name: 'description of this node',\n",
    "                  children: [...],\n",
    "              },\n",
    "              {\n",
    "                  value: 4545,\n",
    "                  name: 'description of this node',\n",
    "                  children: [\n",
    "                      {\n",
    "                          value: 5656,\n",
    "                          name: 'description of this node',\n",
    "                          children: [...]\n",
    "                      },\n",
    "                      ...\n",
    "                  ]\n",
    "              }\n",
    "          ]\n",
    "      },\n",
    "      ...\n",
    "  ]\n",
    "```\n",
    "关于pyecharts中的树形图的文档地址:[pyecharts Tree](http://pyecharts.org/#/zh-cn/charts?id=tree%EF%BC%88%E6%A0%91%E5%9B%BE%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实和我们训练得到的树结构类似，只不过每个结点有个\"name\"属性，表示这个结点的名字，\"value\"表示它的值，\"children\"是一个list，里面还有这样的dict，我们可以写一个递归的函数完成这种数据的生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_echarts_data(tree):\n",
    "    \n",
    "    # 当前顶点的dict\n",
    "    value = dict()\n",
    "    \n",
    "    # 如果传入的tree已经是叶子结点了\n",
    "    if tree['is_leaf'] == True:\n",
    "        \n",
    "        # 它的value就设置为预测的标记\n",
    "        value['value'] = tree['prediction']\n",
    "        \n",
    "        # 它的名字就叫\"label: 标记\"\n",
    "        value['name'] = 'label: %s'%(tree['prediction'])\n",
    "        \n",
    "        # 直接返回这个dict即可\n",
    "        return value\n",
    "    \n",
    "    # 如果传入的tree不是叶子结点，名字就叫当前这个顶点的划分特征，子树是一个list\n",
    "    # 分别增加左子树和右子树到children中\n",
    "    value['name'] = tree['splitting_feature']\n",
    "    value['children'] = [generate_echarts_data(tree['left']), generate_echarts_data(tree['right'])]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = generate_echarts_data(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用下面的代码进行绘制，绘制完成后，树的结点是可以点击的，点击后会展开它的子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    require.config({\n",
       "        paths: {\n",
       "            'echarts': '/nbextensions/echarts/echarts.min'\n",
       "        }\n",
       "    });\n",
       "</script>\n",
       "    <div id=\"3880be47fce940db9c43f03ad736d379\" style=\"width:800px;height:400px;\"></div>\n",
       "\n",
       "\n",
       "<script>\n",
       "    require(['echarts'], function(echarts) {\n",
       "        \n",
       "var myChart_3880be47fce940db9c43f03ad736d379 = echarts.init(document.getElementById('3880be47fce940db9c43f03ad736d379'), 'light', {renderer: 'canvas'});\n",
       "\n",
       "var option_3880be47fce940db9c43f03ad736d379 = {\n",
       "    \"title\": [\n",
       "        {\n",
       "            \"left\": \"auto\",\n",
       "            \"top\": \"auto\",\n",
       "            \"textStyle\": {\n",
       "                \"fontSize\": 18\n",
       "            },\n",
       "            \"subtextStyle\": {\n",
       "                \"fontSize\": 12\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"toolbox\": {\n",
       "        \"show\": true,\n",
       "        \"orient\": \"vertical\",\n",
       "        \"left\": \"95%\",\n",
       "        \"top\": \"center\",\n",
       "        \"feature\": {\n",
       "            \"saveAsImage\": {\n",
       "                \"show\": true,\n",
       "                \"title\": \"save as image\"\n",
       "            },\n",
       "            \"restore\": {\n",
       "                \"show\": true,\n",
       "                \"title\": \"restore\"\n",
       "            },\n",
       "            \"dataView\": {\n",
       "                \"show\": true,\n",
       "                \"title\": \"data view\"\n",
       "            }\n",
       "        }\n",
       "    },\n",
       "    \"series_id\": 3939373,\n",
       "    \"tooltip\": {\n",
       "        \"trigger\": \"item\",\n",
       "        \"triggerOn\": \"mousemove|click\",\n",
       "        \"axisPointer\": {\n",
       "            \"type\": \"line\"\n",
       "        },\n",
       "        \"textStyle\": {\n",
       "            \"fontSize\": 14\n",
       "        },\n",
       "        \"backgroundColor\": \"rgba(50,50,50,0.7)\",\n",
       "        \"borderColor\": \"#333\",\n",
       "        \"borderWidth\": 0\n",
       "    },\n",
       "    \"series\": [\n",
       "        {\n",
       "            \"type\": \"tree\",\n",
       "            \"data\": [\n",
       "                {\n",
       "                    \"name\": \"emp_length_5 years\",\n",
       "                    \"children\": [\n",
       "                        {\n",
       "                            \"name\": \"emp_length_< 1 year\",\n",
       "                            \"children\": [\n",
       "                                {\n",
       "                                    \"name\": \"home_ownership_OWN\",\n",
       "                                    \"children\": [\n",
       "                                        {\n",
       "                                            \"name\": \"emp_length_4 years\",\n",
       "                                            \"children\": [\n",
       "                                                {\n",
       "                                                    \"name\": \"emp_length_9 years\",\n",
       "                                                    \"children\": [\n",
       "                                                        {\n",
       "                                                            \"name\": \"emp_length_8 years\",\n",
       "                                                            \"children\": [\n",
       "                                                                {\n",
       "                                                                    \"value\": 1,\n",
       "                                                                    \"name\": \"label: 1\"\n",
       "                                                                },\n",
       "                                                                {\n",
       "                                                                    \"value\": 1,\n",
       "                                                                    \"name\": \"label: 1\"\n",
       "                                                                }\n",
       "                                                            ]\n",
       "                                                        },\n",
       "                                                        {\n",
       "                                                            \"value\": 1,\n",
       "                                                            \"name\": \"label: 1\"\n",
       "                                                        }\n",
       "                                                    ]\n",
       "                                                },\n",
       "                                                {\n",
       "                                                    \"value\": 1,\n",
       "                                                    \"name\": \"label: 1\"\n",
       "                                                }\n",
       "                                            ]\n",
       "                                        },\n",
       "                                        {\n",
       "                                            \"value\": 1,\n",
       "                                            \"name\": \"label: 1\"\n",
       "                                        }\n",
       "                                    ]\n",
       "                                },\n",
       "                                {\n",
       "                                    \"value\": 1,\n",
       "                                    \"name\": \"label: 1\"\n",
       "                                }\n",
       "                            ],\n",
       "                            \"collapsed\": \"false\"\n",
       "                        },\n",
       "                        {\n",
       "                            \"value\": 1,\n",
       "                            \"name\": \"label: 1\"\n",
       "                        }\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"left\": \"12%\",\n",
       "            \"right\": \"20%\",\n",
       "            \"top\": \"15%\",\n",
       "            \"bottom\": \"12%\",\n",
       "            \"symbol\": \"rect\",\n",
       "            \"symbolSize\": 20,\n",
       "            \"layout\": \"orthogonal\",\n",
       "            \"orient\": \"LR\",\n",
       "            \"label\": {\n",
       "                \"normal\": {\n",
       "                    \"position\": \"left\",\n",
       "                    \"verticalAlign\": \"middle\",\n",
       "                    \"align\": \"right\",\n",
       "                    \"fontSize\": 12,\n",
       "                    \"rotate\": 0\n",
       "                }\n",
       "            },\n",
       "            \"leaves\": {\n",
       "                \"label\": {\n",
       "                    \"normal\": {\n",
       "                        \"position\": \"right\",\n",
       "                        \"verticalAlign\": \"middle\",\n",
       "                        \"align\": \"left\",\n",
       "                        \"fontSize\": 12,\n",
       "                        \"rotate\": 0\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"legend\": [\n",
       "        {\n",
       "            \"data\": [],\n",
       "            \"selectedMode\": \"multiple\",\n",
       "            \"show\": true,\n",
       "            \"left\": \"center\",\n",
       "            \"top\": \"top\",\n",
       "            \"orient\": \"horizontal\",\n",
       "            \"textStyle\": {\n",
       "                \"fontSize\": 12\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"animation\": true,\n",
       "    \"color\": [\n",
       "        \"#c23531\",\n",
       "        \"#2f4554\",\n",
       "        \"#61a0a8\",\n",
       "        \"#d48265\",\n",
       "        \"#749f83\",\n",
       "        \"#ca8622\",\n",
       "        \"#bda29a\",\n",
       "        \"#6e7074\",\n",
       "        \"#546570\",\n",
       "        \"#c4ccd3\",\n",
       "        \"#f05b72\",\n",
       "        \"#ef5b9c\",\n",
       "        \"#f47920\",\n",
       "        \"#905a3d\",\n",
       "        \"#fab27b\",\n",
       "        \"#2a5caa\",\n",
       "        \"#444693\",\n",
       "        \"#726930\",\n",
       "        \"#b2d235\",\n",
       "        \"#6d8346\",\n",
       "        \"#ac6767\",\n",
       "        \"#1d953f\",\n",
       "        \"#6950a1\",\n",
       "        \"#918597\",\n",
       "        \"#f6f5ec\"\n",
       "    ]\n",
       "};\n",
       "myChart_3880be47fce940db9c43f03ad736d379.setOption(option_3880be47fce940db9c43f03ad736d379);\n",
       "\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<pyecharts.charts.tree.Tree at 0x11663fbe0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = Tree(width=800, height=400)\n",
    "tree.add(\"\",\n",
    "         [data],\n",
    "         tree_collapse_interval=5,\n",
    "         tree_top=\"15%\",\n",
    "         tree_right=\"20%\",\n",
    "         tree_symbol = 'rect',\n",
    "         tree_symbol_size = 20,\n",
    "         )\n",
    "tree.render()\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 选做：绘制其他的决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
