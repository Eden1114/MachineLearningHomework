{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二题：神经网络：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验内容：\n",
    "1. 学会梯度下降的基本思想\n",
    "2. 学会使用梯度下降求解线性回归\n",
    "3. 了解归一化处理的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig0.png\" width=300>\n",
    "\n",
    "我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。  \n",
    "我们记输入为$X \\in \\mathbb{R}^{n \\times m}$，输出为$Z \\in \\mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。  \n",
    "输入层到输出层的权重和偏置，我们记为$W \\in \\mathbb{R}^{m}$和$b \\in \\mathbb{R}$。  \n",
    "输出层没有激活函数，所以上面的神经网络的前向传播过程写为：\n",
    "\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "我们使用均方误差作为模型的损失函数\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "我们通过调整参数$W$和$b$来降低均方误差，或者说是以降低均方误差为目标，学习参数$W$和参数$b$。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。\n",
    "\n",
    "在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。\n",
    "\n",
    "在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数：\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。\n",
    "\n",
    "接下来我们会求解损失函数对参数的偏导数。\n",
    "\n",
    "损失函数MSE记为：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$\n",
    "\n",
    "其中，$Z \\in \\mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。\n",
    "\n",
    "我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘：\n",
    "\n",
    "**这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\\n",
    "&= - \\frac{2}{n} X^\\mathrm{T} (y - Z)\\\\\n",
    "&= \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "\\end{aligned}$$\n",
    "\n",
    "同理，求解损失函数对参数$b$的偏导数:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\\n",
    "&= - \\frac{2}{n} \\sum^n_{i=1}(y_i - Z_i)\\\\\n",
    "&= \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。**\n",
    "\n",
    "其中，$\\frac{\\partial \\mathrm{loss}}{\\partial W} \\in \\mathbb{R}^{m}$，$\\frac{\\partial \\mathrm{loss}}{\\partial b} \\in \\mathbb{R}$，求解得到的梯度的维度与参数一致。\n",
    "\n",
    "完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练神经网络的基本思路：\n",
    "\n",
    "1. 首先对参数进行初始化，对参数进行随机初始化（也就是取随机值）\n",
    "2. 将样本输入神经网络，计算神经网络预测值 $Z$\n",
    "3. 计算损失值MSE\n",
    "4. 通过 $Z$ 和 $y$ ，以及 $X$ ，计算参数的梯度\n",
    "5. 使用梯度下降更新参数\n",
    "6. 循环1-5步，**在反复迭代的过程中可以看到损失值不断减小的现象，如果没有下降说明出了问题**\n",
    "\n",
    "接下来我们来实现这个最简单的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用kaggle房价数据，选3列作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('data/kaggle_house_price_prediction/kaggle_hourse_price_train.csv')\n",
    "\n",
    "# 使用这3列作为特征\n",
    "features = ['LotArea', 'BsmtUnfSF', 'GarageArea']\n",
    "target = 'SalePrice'\n",
    "data = data[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40%做测试集，60%做训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(data[features], data[target], test_size = 0.4, random_state = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集876个样本，3个特征，测试集584个样本，3个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((876, 3), (876,), (584, 3), (584,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要初始化参数$W$和$b$，其中$W \\in \\mathbb{R}^m$，$b \\in \\mathbb{R}$，初始化的策略是将$W$初始化成一个随机数矩阵，参数$b$为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(m):\n",
    "    '''\n",
    "    参数初始化，将W初始化成一个随机向量，b是一个长度为1的向量\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m: int, 特征数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    W: np.ndarray, shape = (m, ), 参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, ), 参数b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 指定随机种子，这样生成的随机数就是固定的了，这样就可以与下面的测试样例进行比对\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    W = np.random.normal(size = (m, )) * 0.01\n",
    "    \n",
    "    b = np.zeros((1, ))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要完成输入矩阵$X$在神经网络中的计算，也就是完成 $Z = XW + b$ 的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    '''\n",
    "    前向传播，计算Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 完成Z = XW + b的计算\n",
    "    Z = X.dot(W) + b                    # YOUR CODE HERE\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.37377228144393\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = forward(trainX, Wt, bt)\n",
    "print(tmp.mean()) # -28.37377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来编写损失函数，我们以均方误差(MSE)作为损失函数，需要大家实现MSE的计算：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE，均方误差\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, )，预测值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float，损失值\n",
    "    \n",
    "    '''\n",
    "    # 计算MSE\n",
    "    loss = 0\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        loss += (true - pred) ** 2\n",
    "    loss /= n\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39381033680.5\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = mse(trainY, forward(trainX, Wt, bt))\n",
    "print(tmp) # 39381033680.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们要完成梯度的计算，也就是计算出损失函数对参数的偏导数的导数值：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} = \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} = \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Z, y_true):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n = len(y_true)\n",
    "    \n",
    "    dW = np.dot(X.T, (Z-y_true)) * (2 / n)\n",
    "    db = np.zeros(1,)\n",
    "    db[0] = (2 / n) * (Z.sum() - y_true.sum())\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "-1532030241.25\n",
      "-364308.555764\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print(dWt.shape) # (3,)\n",
    "print(dWt.mean()) # -1532030241.25\n",
    "print(dbt.mean()) # -364308.555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分需要实现梯度下降的函数\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(dW, db, W, b, learning_rate):\n",
    "    '''\n",
    "    梯度下降，参数更新，不需要返回值，W和b实际上是以引用的形式传入到函数内部，\n",
    "    函数内改变W和b会直接影响到它们本身，所以不需要返回值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 更新W\n",
    "    W -= learning_rate * dW\n",
    "    \n",
    "    # 更新b\n",
    "    b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00405243937693\n",
      "0.0\n",
      "(3,)\n",
      "15320302.4166\n",
      "3643.08555764\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 3643.08555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成整个参数更新的过程，先计算梯度，再更新参数，将compute_gradient和update组装在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(X, Z, y_true, W, b, learning_rate):\n",
    "    '''\n",
    "    使用compute_gradient和update函数，先计算梯度，再更新参数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算参数的梯度\n",
    "    dW, db = compute_gradient(X, Z, y_true)                    # YOUR CODE HERE\n",
    "    \n",
    "    # 更新参数\n",
    "    update(dW, db, W, b, learning_rate)                        # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00405243937693\n",
      "0.0\n",
      "(3,)\n",
      "15320302.4166\n",
      "3643.08555764\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "backward(trainX, Zt, trainY, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 3643.08555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, W, b, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播，更新参数\n",
    "    同时记录训练集和测试集上的损失值，后面画图用。然后循环往复，直到达到最大迭代次数epochs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, ), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, )，测试集的标记\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，参数b\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # 这里我们要将神经网络的输出值保存起来，因为后面反向传播的时候需要这个值\n",
    "        Z = forward(trainX, W, b)\n",
    "        \n",
    "        # 计算训练集的损失值\n",
    "        training_loss = mse(trainY, Z)\n",
    "        \n",
    "        # 计算测试集的损失值        \n",
    "        testing_loss = mse(testY, forward(testX, W, b))\n",
    "        \n",
    "        # 将损失值存起来\n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        # 打印损失值，debug用\n",
    "        if verbose:\n",
    "            print('epoch %s training loss: %s'%(epoch+1, training_loss))\n",
    "            print('epoch %s testing loss: %s'%(epoch+1, testing_loss))\n",
    "            print()\n",
    "        \n",
    "        # 反向传播，参数更新\n",
    "        backward(trainX, Z, trainY, W, b, learning_rate)\n",
    "        \n",
    "    return training_loss_list, testing_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00405243937693\n",
      "0.0\n",
      "[39381033680.46006, 3.390230782482132e+23]\n",
      "[38555252685.093849, 4.1516070231815822e+23]\n",
      "-5.70557906009e+13\n",
      "-8824267814.59\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean())          # 0.00405243937693\n",
    "print(bt.mean())          # 0.0\n",
    "\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, 2, learning_rate = 0.01, verbose = False)\n",
    "\n",
    "print(training_loss_list) # [39381033680.460075, 3.3902307664083424e+23]\n",
    "print(testing_loss_list)  # [38555252685.093872, 4.1516070070405267e+23]\n",
    "print(Wt.mean())          # -5.70557904608e+13\n",
    "print(bt.mean())          # -8824267814.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 检查\n",
    "\n",
    "编写一个绘制损失值变化曲线的函数\n",
    "\n",
    "一般我们通过绘制损失函数的变化曲线来判断模型的拟合状态。\n",
    "\n",
    "一般来说，随着迭代轮数的增加，训练集的loss在下降，而测试集的loss在上升，这说明我们正在不断地让模型在训练集上表现得越来越好，在测试集上表现得越来越糟糕，这就是过拟合的体现。  \n",
    "\n",
    "如果训练集loss和测试集loss共同下降，这就是我们想要的结果，说明模型正在很好的学习。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这些函数就是完成整个神经网络需要的函数了\n",
    "\n",
    "|函数名|功能|\n",
    "|-|-|\n",
    "|initialize | 参数初始化|\n",
    "|forward | 给定数据，计算神经网络的输出值|\n",
    "|mse | 给定真值，计算神经网络的预测值与真值之间的差距|\n",
    "|backward | 计算参数的梯度，并实现参数的更新|\n",
    "|compute_gradient | 计算参数的梯度|\n",
    "|update | 参数的更新|\n",
    "|backward | 计算参数梯度，并且更新参数|\n",
    "|train | 训练神经网络|\n",
    "|plot_loss_curve | 绘制损失函数的变化曲线|\n",
    "\n",
    "我们使用参数初始化函数和训练函数，完成神经网络的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss: 39381033680.5\n",
      "epoch 1 testing loss: 38555252685.1\n",
      "\n",
      "epoch 2 training loss: 3.39023078248e+23\n",
      "epoch 2 testing loss: 4.15160702318e+23\n",
      "\n",
      "epoch 3 training loss: 5.05550310435e+36\n",
      "epoch 3 testing loss: 6.19344960711e+36\n",
      "\n",
      "epoch 4 training loss: 7.53876412063e+49\n",
      "epoch 4 testing loss: 9.23567624315e+49\n",
      "\n",
      "epoch 5 training loss: 1.12418019125e+63\n",
      "epoch 5 testing loss: 1.37722365812e+63\n",
      "\n",
      "epoch 6 training loss: 1.67637703232e+76\n",
      "epoch 6 testing loss: 2.05371534458e+76\n",
      "\n",
      "epoch 7 training loss: 2.49981273142e+89\n",
      "epoch 7 testing loss: 3.06249946529e+89\n",
      "\n",
      "epoch 8 training loss: 3.72771970248e+102\n",
      "epoch 8 testing loss: 4.56679792535e+102\n",
      "\n",
      "epoch 9 training loss: 5.5587740656e+115\n",
      "epoch 9 testing loss: 6.81000716158e+115\n",
      "\n",
      "epoch 10 training loss: 8.28924156819e+128\n",
      "epoch 10 testing loss: 1.01550798391e+129\n",
      "\n",
      "epoch 11 training loss: 1.23609135693e+142\n",
      "epoch 11 testing loss: 1.51432508206e+142\n",
      "\n",
      "epoch 12 training loss: 1.8432589159e+155\n",
      "epoch 12 testing loss: 2.25816093078e+155\n",
      "\n",
      "epoch 13 training loss: 2.74866692662e+168\n",
      "epoch 13 testing loss: 3.36736863815e+168\n",
      "\n",
      "epoch 14 training loss: 4.09881097457e+181\n",
      "epoch 14 testing loss: 5.02141870876e+181\n",
      "\n",
      "epoch 15 training loss: 6.11214521577e+194\n",
      "epoch 15 testing loss: 7.4879374842e+194\n",
      "\n",
      "epoch 16 training loss: 9.11442839653e+207\n",
      "epoch 16 testing loss: 1.11660092534e+208\n",
      "\n",
      "epoch 17 training loss: 1.35914318235e+221\n",
      "epoch 17 testing loss: 1.66507483416e+221\n",
      "\n",
      "epoch 18 training loss: 2.02675374666e+234\n",
      "epoch 18 testing loss: 2.48295889824e+234\n",
      "\n",
      "epoch 19 training loss: 3.02229434171e+247\n",
      "epoch 19 testing loss: 3.70258727348e+247\n",
      "\n",
      "epoch 20 training loss: 4.50684406183e+260\n",
      "epoch 20 testing loss: 5.52129659795e+260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特征数m\n",
    "m = trainX.shape[1]\n",
    "\n",
    "# 参数初始化\n",
    "W, b = initialize(m)\n",
    "\n",
    "# 训练20轮，学习率为0.01\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, 20, learning_rate = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失值的变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAF+CAYAAAC1TN9pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110cfda90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印损失的信息我们可以看到损失值持续上升，这就说明哪里出了问题。但是如果所有的测试样例都通过了，就说明我们的实现是没有问题的。运行下面的测试样例，观察哪里出了问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, W: [-0.00348894  0.00983703  0.00580923]\n",
      "epoch 0, b: [ 0.]\n",
      "\n",
      "dWt: [ -4.18172940e+09  -2.19880296e+08  -1.94481031e+08]\n",
      "db: [-364308.55576374]\n",
      "\n",
      "epoch 1, W: [ 41817293.96016914   2198802.97412493   1944810.31544993]\n",
      "epoch 1, b: [ 3643.08555764]\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print('epoch 0, W:', Wt)  # [-0.00348894  0.00983703  0.00580923]\n",
    "print('epoch 0, b:', bt)  # [ 0.]\n",
    "print()\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print('dWt:', dWt) # [ -4.18172940e+09  -2.19880296e+08  -1.94481031e+08]\n",
    "print('db:', dbt) # -364308.555764\n",
    "print()\n",
    "\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "print('epoch 1, W:', Wt)  # [ 41817293.96016914   2198802.97412493   1944810.31544994]\n",
    "print('epoch 1, b:', bt)  # [ 3643.08555764]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们最开始的参数都是在 $10^{-3}$ 这个数量级上，而第一轮迭代时计算出的梯度的数量级在 $10^8$ 左右，这就导致使用梯度下降更新的时候，让参数变成了 $10^6$ 这个数量级左右（学习率为0.01）。产生这样的问题的主要原因是：我们的原始数据 $X$ 没有经过适当的处理，直接扔到了神经网络中进行训练，导致在计算梯度时，由于 $X$ 的数量级过大，导致梯度的数量级变大，在参数更新时使得参数的数量级不断上升，导致参数无法收敛。\n",
    "\n",
    "解决的方法也很简单，对参数进行归一化处理，将其标准化，使均值为0，缩放到 $[-1, 1]$附近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化处理和第一题一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stand = StandardScaler()\n",
    "trainX_normalized = stand.fit_transform(trainX)\n",
    "testX_normalized = stand.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新训练模型，这次我们迭代40轮，学习率设置为0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = trainX.shape[1]\n",
    "W, b = initialize(m)\n",
    "training_loss_list, testing_loss_list = train(trainX_normalized, trainY, testX_normalized, testY, W, b, 40, learning_rate = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAF+CAYAAAAstAbcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110cee9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集上的MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60305.852679101547"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = forward(testX_normalized, W, b)\n",
    "mse(testY, prediction) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
